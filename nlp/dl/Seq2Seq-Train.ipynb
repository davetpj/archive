{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq2Seq-Train.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"15iX2rfd_8pLCYBXtvu8R-ADr9sreBfAZ","authorship_tag":"ABX9TyML9QDJtQp6GUGurWUlbz/a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Ae3mz1g9nCcA"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqdQyLdcpM7P","executionInfo":{"status":"ok","timestamp":1627971186305,"user_tz":-540,"elapsed":5,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5f4qpMPmuML","executionInfo":{"status":"ok","timestamp":1627971306564,"user_tz":-540,"elapsed":326,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"1013bf36-2a35-4ff0-b97c-b0ff3a29ac77"},"source":["!cp '/content/drive/MyDrive/gh/NL_basic/nlp_DL/preprocess.py' ."],"execution_count":4,"outputs":[{"output_type":"stream","text":["cp: cannot stat '/content/drive/MyDrive/gh/NL_basic/nlp_DL/preprocess.py': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3qHjR8PwnD6u","executionInfo":{"status":"ok","timestamp":1627971301887,"user_tz":-540,"elapsed":320,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["!cp -r '/content/drive/MyDrive/gh/NLP_basic/nlp_DL/data_in' ."],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWC0Mnx3nEzJ"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GBf4evR5nF-A"},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import matplotlib.pyplot as plt\n","\n","from preprocess import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSVoQi1UnHKl"},"source":["def plot_graphs(history, string):\n","    plt.plot(history.history[string])\n","    plt.plot(history.history['val_' + string])\n","    plt.xlabel('Epochs')\n","    plt.ylabel(string)\n","    plt.legend([string, 'val_'+ string])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rsyn1A-lnIY0"},"source":["DATA_IN_PATH = './data_in/'\n","DATA_OUT_PATH = './data_out/'\n","TRAIN_INPUTS = 'train_inupts.npy'\n","TRAIN_OUTPUTS = 'train_outputs.npy'\n","TRAIN_TARGETS = 'train_targets.npy'\n","DATA_CONFIG = 'data_configs.json'\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYKwl-V8nJzC"},"source":["SEED_NUM = 42\n","tf.random.set_seed(SEED_NUM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RjkaCmA2nK5u"},"source":["index_inputs = np.load(open(DATA_IN_PATH + TRAIN_INPUTS, 'rb'))\n","index_outputs = np.load(open(DATA_IN_PATH + TRAIN_OUTPUTS, 'rb'))\n","index_targets = np.load(open(DATA_IN_PATH + TRAIN_TARGETS, 'rb'))\n","prepro_config = json.load(open(DATA_IN_PATH + DATA_CONFIG, 'r'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gBzQYq1hnMz-"},"source":["len(index_inputs), len(index_outputs), len(index_targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RvFbSiK3nOEd"},"source":["MODEL_NAME = 'seq2seq_kor'\n","BATCH_SIZE = 24\n","MAX_SEQUENCE = 25\n","EPOCH = 30\n","UNITS = 1024\n","EMBEDDING_DIM = 256\n","VALIDATION_SPLIT = 0.1\n","\n","word2idx = prepro_config['word2idx']\n","idx2word = prepro_config['idx2word']\n","sos_idx = prepro_config['sos_symbol']\n","eos_idx = prepro_config['eos_symbol']\n","vocab_size = prepro_config['vocab_size']\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFPDnHNVnPeE"},"source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n","        super(Encoder, self).__init__()\n","        self.batch_size = batch_size\n","        self.enc_units = enc_units\n","        self.embedding_dim = embedding_dim\n","        self.vocab_size = vocab_size\n","\n","        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences = True , return_state = True, recurrent_initializer='glorot_uniform')\n","\n","\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state=hidden)\n","        return output, state\n","\n","    def initial_hidden_state(self, input):\n","        return tf.zeros((tf.shape(input)[0], self.enc_units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLeAgLlznRTY"},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        hidden_with_time_axis = tf.expand_dims(query, 1)\n","\n","        score = self.V(tf.nn.tanh(\n","            self.W1(values) + self.W2(hidden_with_time_axis)\n","        ))\n","\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        context_vector = attention_weights * values\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFTEQvnknTYL"},"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n","        super(Decoder, self).__init__()\n","\n","        self.batch_size = batch_size\n","        self.dec_units = dec_units\n","        self.embedding_dim = embedding_dim\n","        self.vocab_size = vocab_size\n","\n","        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","\n","        self.fc = tf.keras.layers.Dense(self.vocab_size)\n","\n","        self.attention = BahdanauAttention(self.dec_units)\n","\n","    def call(self, x, hidden, enc_output):\n","        context_vector, attention_weight = self.attention(hidden, enc_output)\n","\n","        x = self.embedding(x)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","        output, state = self.gru(x)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","\n","        x = self.fc(output)\n","\n","        return x, state, attention_weight\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yqdHMB--nT-r"},"source":["optimizer = tf.keras.optimizers.Adam()\n","\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BTtq8pgnWKl"},"source":["def loss(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_fn(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    return tf.reduce_mean(loss_)\n","\n","def accuracy(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n","    pred *= mask\n","    acc = train_accuracy(real, pred)\n","\n","    return tf.reduce_mean(acc)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQxo4-xhnXk5"},"source":["class Seq2Seq(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, dec_units, batch_size, end_token_id = EOS_INDEX ):\n","        super(Seq2Seq, self).__init__()\n","        self.end_token_id = end_token_id\n","        self.encoder = Encoder(vocab_size, embedding_dim, enc_units, batch_size)\n","        self.decoder = Decoder(vocab_size, embedding_dim, dec_units, batch_size)\n","\n","    def call(self, x):\n","        input, target = x\n","\n","        enc_hidden = self.encoder.initial_hidden_state(input)\n","        enc_output, enc_hidden = self.encoder(input, enc_hidden)\n","\n","        dec_hidden = enc_hidden\n","\n","        predict_tokens = []\n","\n","        for t in range(target.shape[1]):\n","            dec_input = tf.dtypes.cast(tf.expand_dims(target[:,t], 1), tf.float32)\n","            predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n","            predict_tokens.append(tf.dtypes.cast(predictions, tf.float32))\n","\n","        return tf.stack(predict_tokens, axis=1)\n","\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5h0cJob4nYzq"},"source":["model = Seq2Seq(vocab_size, EMBEDDING_DIM, UNITS, UNITS, BATCH_SIZE)\n","model.compile(loss=loss, optimizer=optimizer, metrics=[accuracy])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wilArQpBnaDS"},"source":["PATH = DATA_OUT_PATH + MODEL_NAME\n","if not (os.path.isdir(PATH)):\n","    os.makedirs(os.path.join(PATH))\n","\n","checkpoint_path = DATA_OUT_PATH + MODEL_NAME + \"/weights.h5\"\n","\n","cp_callback = ModelCheckpoint( checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n","earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10)\n","\n","history = model.fit([index_inputs, index_outputs], index_targets, batch_size=BATCH_SIZE, epochs=EPOCH, \n","                    validation_split=VALIDATION_SPLIT, callbacks=[earlystop_callback, cp_callback])"],"execution_count":null,"outputs":[]}]}