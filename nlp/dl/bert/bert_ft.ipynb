{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_ft.ipynb","provenance":[],"authorship_tag":"ABX9TyNTrk/cntPo0jhYbBWjufQ1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"48f611c666ba4877bd14a5b71505eea1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3123a0f1e56e43138ec6cb120dc60610","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fda089efada94ff79ec8f66fdaf0f082","IPY_MODEL_7c9e853fa5014be3900c3179970e0c1f"]}},"3123a0f1e56e43138ec6cb120dc60610":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fda089efada94ff79ec8f66fdaf0f082":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_afe601c61fc84fc19f6266992ff68066","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":371391,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":371391,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71f5c7ebca714ea182868771a05c85aa"}},"7c9e853fa5014be3900c3179970e0c1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4ecd854f763d402fb45b0b43babdca43","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 371k/371k [00:03&lt;00:00, 103kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf7bf9314d604e5396f83aaf4704126e"}},"afe601c61fc84fc19f6266992ff68066":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"71f5c7ebca714ea182868771a05c85aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ecd854f763d402fb45b0b43babdca43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bf7bf9314d604e5396f83aaf4704126e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8d59c7e7b532477d9418db03dea2a6af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c608630ab505464e80fa0b6e120c3350","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f6733ca1be8848a68bb5f3fe5e2b50b3","IPY_MODEL_8d687b2949b2464fadee3b20ae81a5ec"]}},"c608630ab505464e80fa0b6e120c3350":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f6733ca1be8848a68bb5f3fe5e2b50b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b201493a8a7f4e0caaeeebc7bca2c55a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":77779,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":77779,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c58068c14f7d486a905f3e77143efd0d"}},"8d687b2949b2464fadee3b20ae81a5ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_809097785a434764bc4ad5b4974c17f7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 77.8k/77.8k [00:02&lt;00:00, 28.4kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_650cf85988074aaca5ab134b85cda85e"}},"b201493a8a7f4e0caaeeebc7bca2c55a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c58068c14f7d486a905f3e77143efd0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"809097785a434764bc4ad5b4974c17f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"650cf85988074aaca5ab134b85cda85e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"54fed20bfeb94455ad3118d0b22018ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3445d765e4034f17bf2cdec6e6491f7e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5c5fdcae3bc944aa835a7b2d7367718b","IPY_MODEL_51485e64fa884265951155137275f71a"]}},"3445d765e4034f17bf2cdec6e6491f7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c5fdcae3bc944aa835a7b2d7367718b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a4233fba981842818424d7e708941cda","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":51,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":51,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fe4aff3d4dd48d29f5323965562eefe"}},"51485e64fa884265951155137275f71a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_af2b50f4f6df4171b3fe6916c6e58806","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 51.0/51.0 [00:01&lt;00:00, 46.9B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0992053310d04c30838f52354284813e"}},"a4233fba981842818424d7e708941cda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7fe4aff3d4dd48d29f5323965562eefe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af2b50f4f6df4171b3fe6916c6e58806":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0992053310d04c30838f52354284813e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b5253cf01324f9b882a05d1ba0fbbdb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d04c6af338074374af6ebc9ce30b5c02","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4d0f59ac0ae245bbb3dd2faf0921c4be","IPY_MODEL_4804cdf1b5a640f582bc79bc0f0ecc67"]}},"d04c6af338074374af6ebc9ce30b5c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d0f59ac0ae245bbb3dd2faf0921c4be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ba09aba7249f477dac20b4179c5b29f8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":426,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":426,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f2a990ecc4194e789279b9fa7d3881a8"}},"4804cdf1b5a640f582bc79bc0f0ecc67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_de7e07d73d8741449b6198d8c66c71aa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 426/426 [00:00&lt;00:00, 2.33kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6cf2ddf217154373950435eb948a3f39"}},"ba09aba7249f477dac20b4179c5b29f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f2a990ecc4194e789279b9fa7d3881a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de7e07d73d8741449b6198d8c66c71aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6cf2ddf217154373950435eb948a3f39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c90e309d5acf4b379f08337fe2cb446d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bb56218809c84450b5e0c11a6859ff49","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_567b6c3b3b164bc3a2bea14c0c9ffa36","IPY_MODEL_d6d2075dde71444c804fced32e020edb"]}},"bb56218809c84450b5e0c11a6859ff49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"567b6c3b3b164bc3a2bea14c0c9ffa36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a5b2e98ef6844f2ab6bfe3ea33978fcc","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":368792146,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":368792146,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fda077dba3d4451ae1a402cc5bbf970"}},"d6d2075dde71444c804fced32e020edb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fc08a119f6b048f5bcfa07d1498ea267","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 369M/369M [00:10&lt;00:00, 33.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_767b845005be42bc9d95798810c3026c"}},"a5b2e98ef6844f2ab6bfe3ea33978fcc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7fda077dba3d4451ae1a402cc5bbf970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc08a119f6b048f5bcfa07d1498ea267":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"767b845005be42bc9d95798810c3026c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_V2bU05KdF5","executionInfo":{"status":"ok","timestamp":1628823635427,"user_tz":-540,"elapsed":7001,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"3205fd52-5c16-4ce3-9079-f1ca1519eb65"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 15.3 MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 48.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 78.6 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 67.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p66l4kfXLKCz","executionInfo":{"status":"ok","timestamp":1628823638492,"user_tz":-540,"elapsed":3068,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"c854d849-9212-46ea-c988-35b443b099de"},"source":["!pip install sentencepiece"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 12.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SVKAvhdeLWgG","executionInfo":{"status":"ok","timestamp":1628823643149,"user_tz":-540,"elapsed":4670,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["import os\n","import json\n","\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from transformers import *\n","from tqdm import tqdm\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCLrh63kMAHp","executionInfo":{"status":"ok","timestamp":1628823650335,"user_tz":-540,"elapsed":7201,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"11bfc1af-4f54-478a-e677-32760579b044"},"source":["!git clone https://github.com/e9t/nsmc.git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'nsmc'...\n","remote: Enumerating objects: 14763, done.\u001b[K\n","remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n","Receiving objects: 100% (14763/14763), 56.19 MiB | 21.06 MiB/s, done.\n","Resolving deltas: 100% (1749/1749), done.\n","Checking out files: 100% (14737/14737), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75elSRzEMOB2","executionInfo":{"status":"ok","timestamp":1628823650337,"user_tz":-540,"elapsed":11,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"192245f4-fda6-4fe7-91ba-2115ebf2a668"},"source":["!ls nsmc"],"execution_count":5,"outputs":[{"output_type":"stream","text":["code\t\t  ratings_train.txt  raw\tsynopses.json\n","ratings_test.txt  ratings.txt\t     README.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NqO9KavqMRzB","executionInfo":{"status":"ok","timestamp":1628823650917,"user_tz":-540,"elapsed":586,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["train = pd.read_table(\"nsmc/\" + \"ratings_train.txt\")\n","test = pd.read_table(\"nsmc/\" + \"ratings_test.txt\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"41X4VdFQMkPD","executionInfo":{"status":"ok","timestamp":1628823650926,"user_tz":-540,"elapsed":17,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"382338a3-eb73-4559-c928-738153b94cd6"},"source":["train[50:70]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>50</th>\n","      <td>9063648</td>\n","      <td>영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>8272095</td>\n","      <td>야 세르게이! 작은고추의 매운맛을 보여주마! 포퐁저그 콩진호가 간다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>2345905</td>\n","      <td>이렇게 가슴시리게 본 드라마가 또 있을까? 감동 그 자체!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>7865630</td>\n","      <td>난또 저 꼬마애가 무슨 원한이 깊길래.,. 했더니 OO 그냥 혼자 나대다 OO걸 어...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>7207064</td>\n","      <td>재미있어요</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>5719655</td>\n","      <td>전 좋아요</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>1651126</td>\n","      <td>최고</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>7246040</td>\n","      <td>너무 충격적이엇다. 기분을 완전히 푹 꺼지게 하는 느낌... 활력이라고는 하나도 없...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>717775</td>\n","      <td>심심한영화.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>8317483</td>\n","      <td>백봉기 언제나오나요?</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>1031725</td>\n","      <td>보는내내 그대로 들어맞는 예측 카리스마 없는 악역</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>3993146</td>\n","      <td>불알이 나와서 당황...아무튼 영화가 중간에 끝나는 느낌</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>2196616</td>\n","      <td>평범함속에 녹아든 평범한 일상. 조금 밋밋한게 흠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>8203798</td>\n","      <td>보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>2332588</td>\n","      <td>사랑하고싶게하는,가슴속온감정을헤집어놓는영화예요정말최고.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>10084753</td>\n","      <td>많은 사람들이 이 다큐를 보고 우리나라 슬픈 현대사의 한 단면에 대해 깊이 생각하고...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>8518645</td>\n","      <td>예전 작품 캐릭터, 에피소드 재탕 삼탕 사골우려먹듯 우리고 내용은 산으로 가고 시청...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>7956793</td>\n","      <td>김남길의 백점짜리 연기력과 초반 몰입도에도 불구하고 지루하고 손예진 ㅈㅈ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>3996917</td>\n","      <td>재밌네 비슷한 영화를 안보신 분들한테는 재미있을 듯</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>8128006</td>\n","      <td>노래실력으로뽑는게 맞냐? 박시환이 mama나가면 진짜 망신이다</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id                                           document  label\n","50   9063648  영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영...      1\n","51   8272095              야 세르게이! 작은고추의 매운맛을 보여주마! 포퐁저그 콩진호가 간다      0\n","52   2345905                   이렇게 가슴시리게 본 드라마가 또 있을까? 감동 그 자체!      1\n","53   7865630  난또 저 꼬마애가 무슨 원한이 깊길래.,. 했더니 OO 그냥 혼자 나대다 OO걸 어...      0\n","54   7207064                                              재미있어요      1\n","55   5719655                                              전 좋아요      1\n","56   1651126                                                 최고      0\n","57   7246040  너무 충격적이엇다. 기분을 완전히 푹 꺼지게 하는 느낌... 활력이라고는 하나도 없...      1\n","58    717775                                             심심한영화.      0\n","59   8317483                                        백봉기 언제나오나요?      1\n","60   1031725                        보는내내 그대로 들어맞는 예측 카리스마 없는 악역      0\n","61   3993146                    불알이 나와서 당황...아무튼 영화가 중간에 끝나는 느낌      0\n","62   2196616                       평범함속에 녹아든 평범한 일상. 조금 밋밋한게 흠.      0\n","63   8203798  보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에...      0\n","64   2332588                     사랑하고싶게하는,가슴속온감정을헤집어놓는영화예요정말최고.      1\n","65  10084753  많은 사람들이 이 다큐를 보고 우리나라 슬픈 현대사의 한 단면에 대해 깊이 생각하고...      1\n","66   8518645  예전 작품 캐릭터, 에피소드 재탕 삼탕 사골우려먹듯 우리고 내용은 산으로 가고 시청...      0\n","67   7956793           김남길의 백점짜리 연기력과 초반 몰입도에도 불구하고 지루하고 손예진 ㅈㅈ      0\n","68   3996917                       재밌네 비슷한 영화를 안보신 분들한테는 재미있을 듯      1\n","69   8128006                 노래실력으로뽑는게 맞냐? 박시환이 mama나가면 진짜 망신이다      0"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Q3NolFrhMnoi","executionInfo":{"status":"ok","timestamp":1628823650927,"user_tz":-540,"elapsed":16,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["# coding=utf-8\n","# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" Tokenization classes for KoBert model.\"\"\"\n","\n","\n","import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n","\n","from transformers import PreTrainedTokenizer\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n","                     \"vocab_txt\": \"vocab.txt\"}\n","\n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n","    }\n","}\n","\n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512\n","}\n","\n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\n","}\n","\n","SPIECE_UNDERLINE = u'▁'\n","\n","\n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","        SentencePiece based tokenizer. Peculiarities:\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","\n","    def __init__(\n","            self,\n","            vocab_file,\n","            vocab_txt,\n","            do_lower_case=False,\n","            remove_space=True,\n","            keep_accents=False,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","            pad_token=\"[PAD]\",\n","            cls_token=\"[CLS]\",\n","            mask_token=\"[MASK]\",\n","            **kwargs):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs\n","        )\n","\n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n","\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","\n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n","\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n","\n","    def get_vocab(self):\n","        return dict(self.token2idx, **self.added_tokens_encoder)\n","\n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n","\n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n","\n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n","\n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize('NFKD', outputs) # 유니코드 등가성 정준 분해?\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n","\n","        return outputs\n","\n","    def _tokenize(self, text, return_unicode=True, sample=False):\n","        \"\"\" Tokenize a string. \"\"\"\n","        text = self.preprocess_text(text)\n","\n","        if not sample:\n","            pieces = self.sp_model.EncodeAsPieces(text)\n","        else:\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n","\n","        return new_pieces\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n","\n","    def _convert_id_to_token(self, index, return_unicode=True):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n","\n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n","\n","    ## 주요\n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A KoBERT sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n","\n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n","\n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n","\n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n","\n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A KoBERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n","\n","    def save_vocabulary(self, save_directory):\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n","            to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n","\n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n","\n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n","\n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n","\n","        return out_vocab_model, out_vocab_txt"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284,"referenced_widgets":["48f611c666ba4877bd14a5b71505eea1","3123a0f1e56e43138ec6cb120dc60610","fda089efada94ff79ec8f66fdaf0f082","7c9e853fa5014be3900c3179970e0c1f","afe601c61fc84fc19f6266992ff68066","71f5c7ebca714ea182868771a05c85aa","4ecd854f763d402fb45b0b43babdca43","bf7bf9314d604e5396f83aaf4704126e","8d59c7e7b532477d9418db03dea2a6af","c608630ab505464e80fa0b6e120c3350","f6733ca1be8848a68bb5f3fe5e2b50b3","8d687b2949b2464fadee3b20ae81a5ec","b201493a8a7f4e0caaeeebc7bca2c55a","c58068c14f7d486a905f3e77143efd0d","809097785a434764bc4ad5b4974c17f7","650cf85988074aaca5ab134b85cda85e","54fed20bfeb94455ad3118d0b22018ed","3445d765e4034f17bf2cdec6e6491f7e","5c5fdcae3bc944aa835a7b2d7367718b","51485e64fa884265951155137275f71a","a4233fba981842818424d7e708941cda","7fe4aff3d4dd48d29f5323965562eefe","af2b50f4f6df4171b3fe6916c6e58806","0992053310d04c30838f52354284813e","2b5253cf01324f9b882a05d1ba0fbbdb","d04c6af338074374af6ebc9ce30b5c02","4d0f59ac0ae245bbb3dd2faf0921c4be","4804cdf1b5a640f582bc79bc0f0ecc67","ba09aba7249f477dac20b4179c5b29f8","f2a990ecc4194e789279b9fa7d3881a8","de7e07d73d8741449b6198d8c66c71aa","6cf2ddf217154373950435eb948a3f39"]},"id":"xo2IbfecP0zl","executionInfo":{"status":"ok","timestamp":1628823655541,"user_tz":-540,"elapsed":4629,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"d5de504d-4390-4840-9188-2b2bb89f92b3"},"source":["tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48f611c666ba4877bd14a5b71505eea1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=371391.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d59c7e7b532477d9418db03dea2a6af","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54fed20bfeb94455ad3118d0b22018ed","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=51.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b5253cf01324f9b882a05d1ba0fbbdb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=426.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KoBertTokenizer'.\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sbegi0QP-vD","executionInfo":{"status":"ok","timestamp":1628823655542,"user_tz":-540,"elapsed":18,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"e8dfd9a9-a7ef-464a-9b2d-18815af5b045"},"source":["print(tokenizer.encode(\"가슴속온감정을헤집어놓는영화예요정말최고.\"))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[2, 755, 6615, 6971, 5341, 7227, 7088, 7895, 7354, 6855, 5737, 5760, 6954, 6957, 6999, 7227, 6160, 7459, 54, 3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"3XrC82KvQYUx","executionInfo":{"status":"ok","timestamp":1628823655543,"user_tz":-540,"elapsed":17,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"99b4b1c8-8fbb-441b-e6a2-99b77f0083b8"},"source":["tokenizer.decode([2, 755, 6615, 6971, 5341, 7227, 7088, 7895, 7354, 6855, 5737, 5760, 6954, 6957, 6999, 7227, 6160, 7459, 54, 3])"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[CLS] 가슴속온감정을헤집어놓는영화예요정말최고.[SEP]'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"SNYRBGQqTw9F","executionInfo":{"status":"ok","timestamp":1628823655544,"user_tz":-540,"elapsed":15,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["SEQ_LEN = 64\n","BATCH_SIZE = 32\n","DATA_COLUMN = 'document'\n","LABEL_COLUMN = 'label'\n","\n","def convert_data(data_df):\n","    \n","\n","    tokens, masks, segments, targets = [], [], [], []\n","\n","    for i in tqdm(range(len(data_df))):\n","        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n","\n","        num_zeros = token.count(1)\n","        mask = [1]*(SEQ_LEN - num_zeros) + [0]*num_zeros\n","\n","        segment = [0]*SEQ_LEN\n","\n","        tokens.append(token)\n","        masks.append(mask)\n","        segments.append(segment)\n","\n","        targets.append(data_df[LABEL_COLUMN][i])\n","\n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    targets = np.array(targets)\n","\n","    return [tokens, masks, segments], targets\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0LF0P0AXYrE","executionInfo":{"status":"ok","timestamp":1628823691577,"user_tz":-540,"elapsed":36048,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"3ab73b6e-db59-4606-a010-009783805ee5"},"source":["def load_data(data):\n","    data_df = data\n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n","    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n","    data_x, data_y = convert_data(data_df)\n","\n","    return data_x, data_y\n","\n","train_x, train_y = load_data(train)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/150000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 150000/150000 [00:33<00:00, 4516.15it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOomvhBiYNlF","executionInfo":{"status":"ok","timestamp":1628823703209,"user_tz":-540,"elapsed":11640,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"44324d85-3d4a-474e-a2c5-8602f7bf8896"},"source":["test_x, test_y = load_data(test)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/50000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 50000/50000 [00:10<00:00, 4695.44it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":134,"referenced_widgets":["c90e309d5acf4b379f08337fe2cb446d","bb56218809c84450b5e0c11a6859ff49","567b6c3b3b164bc3a2bea14c0c9ffa36","d6d2075dde71444c804fced32e020edb","a5b2e98ef6844f2ab6bfe3ea33978fcc","7fda077dba3d4451ae1a402cc5bbf970","fc08a119f6b048f5bcfa07d1498ea267","767b845005be42bc9d95798810c3026c"]},"id":"3bFZI-z6YeFk","executionInfo":{"status":"ok","timestamp":1628823717717,"user_tz":-540,"elapsed":14531,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"57eebbab-df94-413e-c265-f465d9860768"},"source":["model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c90e309d5acf4b379f08337fe2cb446d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=368792146.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["All PyTorch model weights were used when initializing TFBertModel.\n","\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4GXXujUdZRXe","executionInfo":{"status":"ok","timestamp":1628823754075,"user_tz":-540,"elapsed":36379,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"60cad0ae-4189-439f-c295-291295e02047"},"source":["token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n","mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n","segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segments')\n","\n","bert_output = model([token_inputs, mask_inputs, segment_inputs])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fcbfb53fd70>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fcbfb53fd70>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fcc166f2170> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING: AutoGraph could not transform <function wrap at 0x7fcc166f2170> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"teGN6UvzaYig","executionInfo":{"status":"ok","timestamp":1628823754076,"user_tz":-540,"elapsed":33,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"925c6903-54c4-48f4-fbb6-2643f31b34f6"},"source":["bert_output"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TFBaseModelOutputWithPooling([('last_hidden_state',\n","                               <KerasTensor: shape=(None, 64, 768) dtype=float32 (created by layer 'tf_bert_model')>),\n","                              ('pooler_output',\n","                               <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model')>)])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"Y8IMYgtCa3kw","executionInfo":{"status":"ok","timestamp":1628823754077,"user_tz":-540,"elapsed":29,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["bert_output = bert_output[1] # [CLS] 만 가져오겠다? "],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9wbjlbRbGNT","executionInfo":{"status":"ok","timestamp":1628823754078,"user_tz":-540,"elapsed":28,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["learning_rate = 0.001\n","\n","lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate,2344*4,0.5,staircase=True)\n","opt = tf.keras.optimizers.Adam(learning_rate=lr_decay)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHiH3JUhbZ22","executionInfo":{"status":"ok","timestamp":1628823754079,"user_tz":-540,"elapsed":29,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_output)\n","sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid',kernel_initializer=tf.keras.initializers.truncated_normal(stddev=0.02))(sentiment_drop)\n","sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZIodrjRcTMs","executionInfo":{"status":"ok","timestamp":1628823754079,"user_tz":-540,"elapsed":28,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}}},"source":["sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IcQ1epC_cqB_","executionInfo":{"status":"ok","timestamp":1628835851902,"user_tz":-540,"elapsed":12097850,"user":{"displayName":"­김동후","photoUrl":"","userId":"05428640934514761408"}},"outputId":"18d2626d-e517-42d4-917e-4f2d389669c9"},"source":["sentiment_model.fit(train_x,train_y,epochs=15,shuffle=True,batch_size=BATCH_SIZE,validation_data=(test_x,test_y))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","4688/4688 [==============================] - ETA: 0s - loss: 0.6972 - accuracy: 0.5013WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","4688/4688 [==============================] - 823s 173ms/step - loss: 0.6972 - accuracy: 0.5013 - val_loss: 0.6968 - val_accuracy: 0.4965\n","Epoch 2/15\n","4688/4688 [==============================] - 804s 172ms/step - loss: 0.6943 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.5035\n","Epoch 3/15\n","4688/4688 [==============================] - 803s 171ms/step - loss: 0.6937 - accuracy: 0.4998 - val_loss: 0.6942 - val_accuracy: 0.4965\n","Epoch 4/15\n","4688/4688 [==============================] - 804s 171ms/step - loss: 0.6937 - accuracy: 0.4987 - val_loss: 0.6941 - val_accuracy: 0.4965\n","Epoch 5/15\n","4688/4688 [==============================] - 804s 171ms/step - loss: 0.6934 - accuracy: 0.5015 - val_loss: 0.6935 - val_accuracy: 0.5035\n","Epoch 6/15\n","4688/4688 [==============================] - 804s 171ms/step - loss: 0.6934 - accuracy: 0.4987 - val_loss: 0.6934 - val_accuracy: 0.5035\n","Epoch 7/15\n","4688/4688 [==============================] - 803s 171ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6937 - val_accuracy: 0.4965\n","Epoch 8/15\n","4688/4688 [==============================] - 803s 171ms/step - loss: 0.6933 - accuracy: 0.4997 - val_loss: 0.6932 - val_accuracy: 0.4965\n","Epoch 9/15\n","4688/4688 [==============================] - 801s 171ms/step - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6931 - val_accuracy: 0.5035\n","Epoch 10/15\n","4688/4688 [==============================] - 802s 171ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6934 - val_accuracy: 0.4965\n","Epoch 11/15\n","4688/4688 [==============================] - 802s 171ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5035\n","Epoch 12/15\n","4688/4688 [==============================] - 803s 171ms/step - loss: 0.6932 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.4965\n","Epoch 13/15\n","4688/4688 [==============================] - 803s 171ms/step - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.4965\n","Epoch 14/15\n","4688/4688 [==============================] - 803s 171ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.5035\n","Epoch 15/15\n","4688/4688 [==============================] - 803s 171ms/step - loss: 0.6932 - accuracy: 0.4979 - val_loss: 0.6931 - val_accuracy: 0.5035\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fcb47bccc90>"]},"metadata":{"tags":[]},"execution_count":22}]}]}