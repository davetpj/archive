{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"nlp","language":"python","name":"nlp"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"BERT_NSMC_Tensorflow (1).ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ODI5DsuZ8A6R"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from transformers import *\n","import json\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHzSZn3h8A6V","outputId":"0f0bdfd0-ff09-4ff1-ed18-e33bdb688c92"},"source":["!git clone https://github.com/e9t/nsmc.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'nsmc'...\n","remote: Enumerating objects: 14763, done.\u001b[K\n","remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n","Receiving objects: 100% (14763/14763), 56.19 MiB | 21.85 MiB/s, done.\n","Resolving deltas: 100% (1749/1749), done.\n","Checking connectivity... done.\n","Checking out files: 100% (14737/14737), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8PaoGTrY8A6X","outputId":"57b7845a-f462-4d13-cfab-ec2745c8e028"},"source":["os.listdir('nsmc')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.git',\n"," 'README.md',\n"," 'code',\n"," 'ratings.txt',\n"," 'ratings_test.txt',\n"," 'ratings_train.txt',\n"," 'raw',\n"," 'synopses.json']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"ypC71c6f8A6Y"},"source":["train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n","test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZS38c5f8A6Z","outputId":"3938ae91-f5a5-492d-94e2-11fddf73dafa"},"source":["train[50:70]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>50</th>\n","      <td>9063648</td>\n","      <td>영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>8272095</td>\n","      <td>야 세르게이! 작은고추의 매운맛을 보여주마! 포퐁저그 콩진호가 간다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>2345905</td>\n","      <td>이렇게 가슴시리게 본 드라마가 또 있을까? 감동 그 자체!</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>7865630</td>\n","      <td>난또 저 꼬마애가 무슨 원한이 깊길래.,. 했더니 OO 그냥 혼자 나대다 OO걸 어...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>7207064</td>\n","      <td>재미있어요</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>5719655</td>\n","      <td>전 좋아요</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>1651126</td>\n","      <td>최고</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>7246040</td>\n","      <td>너무 충격적이엇다. 기분을 완전히 푹 꺼지게 하는 느낌... 활력이라고는 하나도 없...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>717775</td>\n","      <td>심심한영화.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>8317483</td>\n","      <td>백봉기 언제나오나요?</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>1031725</td>\n","      <td>보는내내 그대로 들어맞는 예측 카리스마 없는 악역</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>3993146</td>\n","      <td>불알이 나와서 당황...아무튼 영화가 중간에 끝나는 느낌</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>2196616</td>\n","      <td>평범함속에 녹아든 평범한 일상. 조금 밋밋한게 흠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>8203798</td>\n","      <td>보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>2332588</td>\n","      <td>사랑하고싶게하는,가슴속온감정을헤집어놓는영화예요정말최고.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>10084753</td>\n","      <td>많은 사람들이 이 다큐를 보고 우리나라 슬픈 현대사의 한 단면에 대해 깊이 생각하고...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>8518645</td>\n","      <td>예전 작품 캐릭터, 에피소드 재탕 삼탕 사골우려먹듯 우리고 내용은 산으로 가고 시청...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>67</th>\n","      <td>7956793</td>\n","      <td>김남길의 백점짜리 연기력과 초반 몰입도에도 불구하고 지루하고 손예진 ㅈㅈ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>68</th>\n","      <td>3996917</td>\n","      <td>재밌네 비슷한 영화를 안보신 분들한테는 재미있을 듯</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>69</th>\n","      <td>8128006</td>\n","      <td>노래실력으로뽑는게 맞냐? 박시환이 mama나가면 진짜 망신이다</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id                                           document  label\n","50   9063648  영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영...      1\n","51   8272095              야 세르게이! 작은고추의 매운맛을 보여주마! 포퐁저그 콩진호가 간다      0\n","52   2345905                   이렇게 가슴시리게 본 드라마가 또 있을까? 감동 그 자체!      1\n","53   7865630  난또 저 꼬마애가 무슨 원한이 깊길래.,. 했더니 OO 그냥 혼자 나대다 OO걸 어...      0\n","54   7207064                                              재미있어요      1\n","55   5719655                                              전 좋아요      1\n","56   1651126                                                 최고      0\n","57   7246040  너무 충격적이엇다. 기분을 완전히 푹 꺼지게 하는 느낌... 활력이라고는 하나도 없...      1\n","58    717775                                             심심한영화.      0\n","59   8317483                                        백봉기 언제나오나요?      1\n","60   1031725                        보는내내 그대로 들어맞는 예측 카리스마 없는 악역      0\n","61   3993146                    불알이 나와서 당황...아무튼 영화가 중간에 끝나는 느낌      0\n","62   2196616                       평범함속에 녹아든 평범한 일상. 조금 밋밋한게 흠.      0\n","63   8203798  보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에...      0\n","64   2332588                     사랑하고싶게하는,가슴속온감정을헤집어놓는영화예요정말최고.      1\n","65  10084753  많은 사람들이 이 다큐를 보고 우리나라 슬픈 현대사의 한 단면에 대해 깊이 생각하고...      1\n","66   8518645  예전 작품 캐릭터, 에피소드 재탕 삼탕 사골우려먹듯 우리고 내용은 산으로 가고 시청...      0\n","67   7956793           김남길의 백점짜리 연기력과 초반 몰입도에도 불구하고 지루하고 손예진 ㅈㅈ      0\n","68   3996917                       재밌네 비슷한 영화를 안보신 분들한테는 재미있을 듯      1\n","69   8128006                 노래실력으로뽑는게 맞냐? 박시환이 mama나가면 진짜 망신이다      0"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"YVqLOVuf8A6Z"},"source":["import logging\n","import os\n","import unicodedata\n","from shutil import copyfile\n"," \n","from transformers import PreTrainedTokenizer\n"," \n"," \n","logger = logging.getLogger(__name__)\n"," \n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n","                     \"vocab_txt\": \"vocab.txt\"}\n"," \n","PRETRAINED_VOCAB_FILES_MAP = {\n","    \"vocab_file\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n","    },\n","    \"vocab_txt\": {\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n","    }\n","}\n"," \n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n","    \"monologg/kobert\": 512,\n","    \"monologg/kobert-lm\": 512,\n","    \"monologg/distilkobert\": 512\n","}\n"," \n","PRETRAINED_INIT_CONFIGURATION = {\n","    \"monologg/kobert\": {\"do_lower_case\": False},\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\n","}\n"," \n","SPIECE_UNDERLINE = u'▁'\n"," \n"," \n","class KoBertTokenizer(PreTrainedTokenizer):\n","    \"\"\"\n","        SentencePiece based tokenizer. Peculiarities:\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n","    \"\"\"\n","    vocab_files_names = VOCAB_FILES_NAMES\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n"," \n","    def __init__(\n","            self,\n","            vocab_file,\n","            vocab_txt,\n","            do_lower_case=False,\n","            remove_space=True,\n","            keep_accents=False,\n","            unk_token=\"[UNK]\",\n","            sep_token=\"[SEP]\",\n","            pad_token=\"[PAD]\",\n","            cls_token=\"[CLS]\",\n","            mask_token=\"[MASK]\",\n","            **kwargs):\n","        super().__init__(\n","            unk_token=unk_token,\n","            sep_token=sep_token,\n","            pad_token=pad_token,\n","            cls_token=cls_token,\n","            mask_token=mask_token,\n","            **kwargs\n","        )\n"," \n","        # Build vocab\n","        self.token2idx = dict()\n","        self.idx2token = []\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\n","            for idx, token in enumerate(f):\n","                token = token.strip()\n","                self.token2idx[token] = idx\n","                self.idx2token.append(token)\n"," \n","        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n","        self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n"," \n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n"," \n","        self.do_lower_case = do_lower_case\n","        self.remove_space = remove_space\n","        self.keep_accents = keep_accents\n","        self.vocab_file = vocab_file\n","        self.vocab_txt = vocab_txt\n"," \n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n"," \n","    @property\n","    def vocab_size(self):\n","        return len(self.idx2token)\n"," \n","    def __getstate__(self):\n","        state = self.__dict__.copy()\n","        state[\"sp_model\"] = None\n","        return state\n"," \n","    def __setstate__(self, d):\n","        self.__dict__ = d\n","        try:\n","            import sentencepiece as spm\n","        except ImportError:\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n","                           \"pip install sentencepiece\")\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(self.vocab_file)\n"," \n","    def preprocess_text(self, inputs):\n","        if self.remove_space:\n","            outputs = \" \".join(inputs.strip().split())\n","        else:\n","            outputs = inputs\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n"," \n","        if not self.keep_accents:\n","            outputs = unicodedata.normalize('NFKD', outputs)\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n","        if self.do_lower_case:\n","            outputs = outputs.lower()\n"," \n","        return outputs\n"," \n","    def _tokenize(self, text, return_unicode=True, sample=False):\n","        \"\"\" Tokenize a string. \"\"\"\n","        text = self.preprocess_text(text)\n"," \n","        if not sample:\n","            pieces = self.sp_model.EncodeAsPieces(text)\n","        else:\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n","        new_pieces = []\n","        for piece in pieces:\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n","                    if len(cur_pieces[0]) == 1:\n","                        cur_pieces = cur_pieces[1:]\n","                    else:\n","                        cur_pieces[0] = cur_pieces[0][1:]\n","                cur_pieces.append(piece[-1])\n","                new_pieces.extend(cur_pieces)\n","            else:\n","                new_pieces.append(piece)\n"," \n","        return new_pieces\n"," \n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\n"," \n","    def _convert_id_to_token(self, index, return_unicode=True):\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n","        return self.idx2token[index]\n"," \n","    def convert_tokens_to_string(self, tokens):\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n","        return out_string\n"," \n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n","        by concatenating and adding special tokens.\n","        A RoBERTa sequence has the following format:\n","            single sequence: [CLS] X [SEP]\n","            pair of sequences: [CLS] A [SEP] B [SEP]\n","        \"\"\"\n","        if token_ids_1 is None:\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        sep = [self.sep_token_id]\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\n"," \n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n","        \"\"\"\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n","        Args:\n","            token_ids_0: list of ids (must not contain special tokens)\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n","                for sequence pairs\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n","                special tokens for the model\n","        Returns:\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n","        \"\"\"\n"," \n","        if already_has_special_tokens:\n","            if token_ids_1 is not None:\n","                raise ValueError(\n","                    \"You should not supply a second sequence if the provided sequence of \"\n","                    \"ids is already formated with special tokens for the model.\"\n","                )\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n"," \n","        if token_ids_1 is not None:\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n","        return [1] + ([0] * len(token_ids_0)) + [1]\n"," \n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n","        \"\"\"\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n","        A BERT sequence pair mask has the following format:\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n","        | first sequence    | second sequence\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\n","        \"\"\"\n","        sep = [self.sep_token_id]\n","        cls = [self.cls_token_id]\n","        if token_ids_1 is None:\n","            return len(cls + token_ids_0 + sep) * [0]\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n"," \n","    def save_vocabulary(self, save_directory):\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n","            to a directory.\n","        \"\"\"\n","        if not os.path.isdir(save_directory):\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n","            return\n"," \n","        # 1. Save sentencepiece model\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n"," \n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n","            copyfile(self.vocab_file, out_vocab_model)\n"," \n","        # 2. Save vocab.txt\n","        index = 0\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n","                if index != token_index:\n","                    logger.warning(\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n","                    )\n","                    index = token_index\n","                writer.write(token + \"\\n\")\n","                index += 1\n"," \n","        return out_vocab_model, out_vocab_txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPfa0ptA8A6e","outputId":"e22c0d8c-0dd5-4e44-dbee-5b0f35e11787"},"source":["tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n","Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"t4G6uiTk8A6g","outputId":"4083f863-1743-45fc-e6fc-2a8d78a4850f"},"source":["print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2, 2366, 5678, 5678, 1192, 1804, 6166, 5760, 3415, 4638, 3272, 3133, 6926, 3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8B0FB7M88A6h","outputId":"e7db9c4a-3209-4798-905b-f57cbf80027b"},"source":["valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n","print(valid_num * [1] + (64 - valid_num) * [0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bMbrCuo38A6i","outputId":"44896ef8-cbb0-4d50-d83b-196725ca1f88"},"source":["def convert_data(data_df):\n","    global tokenizer\n","    \n","    SEQ_LEN = 64 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n","    \n","    tokens, masks, segments, targets = [], [], [], []\n","    \n","    for i in tqdm(range(len(data_df))):\n","        # token : 문장을 토큰화함\n","        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n","       \n","        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n","        num_zeros = token.count(0)\n","        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n","        \n","        # 문장의 전후관계를 구분해주는 세그먼트는 문장이 1개밖에 없으므로 모두 0\n","        segment = [0]*SEQ_LEN\n"," \n","        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n","        tokens.append(token)\n","        masks.append(mask)\n","        segments.append(segment)\n","        \n","        # 정답(긍정 : 1 부정 0)을 targets 변수에 저장해 줌\n","        targets.append(data_df[LABEL_COLUMN][i])\n"," \n","    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정    \n","    tokens = np.array(tokens)\n","    masks = np.array(masks)\n","    segments = np.array(segments)\n","    targets = np.array(targets)\n"," \n","    return [tokens, masks, segments], targets\n"," \n","# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n","def load_data(pandas_dataframe):\n","    data_df = pandas_dataframe\n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n","    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n","    data_x, data_y = convert_data(data_df)\n","    return data_x, data_y\n"," \n","SEQ_LEN = 64\n","BATCH_SIZE = 32\n","# 긍부정 문장을 포함하고 있는 칼럼\n","DATA_COLUMN = \"document\"\n","# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n","LABEL_COLUMN = \"label\"\n"," \n","# train 데이터를 버트 인풋에 맞게 변환\n","train_x, train_y = load_data(train)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 150000/150000 [00:33<00:00, 4503.36it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"G2AD2ba38A6j","outputId":"9e0909ac-e075-48cc-91ec-7e77cd0da6a7"},"source":["test_x, test_y = load_data(test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 50000/50000 [00:10<00:00, 4573.72it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2oOKVpsZ8A6j"},"source":["# 버트 훈련을 빠르게 하기 위해, TPU를 사용하도록 하겠습니다\n","model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n","# 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의\n","token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n","mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n","segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n","# 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의\n","bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UHfWCxN8A6k","outputId":"7b0ccdf4-5b2e-460c-8744-40b47d86f8ba"},"source":["bert_outputs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor 'tf_bert_model/Identity:0' shape=(None, 64, 768) dtype=float32>,\n"," <tf.Tensor 'tf_bert_model/Identity_1:0' shape=(None, 768) dtype=float32>)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"FqDLiCg18A6l"},"source":["bert_outputs = bert_outputs[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoervOCy8A6m","outputId":"1b888862-d2e4-40a9-8c94-3b837a571b7b"},"source":["# # Rectified Adam 옵티마이저 사용\n","# import tensorflow_addons as tfa\n","# # 총 batch size * 4 epoch = 2344 * 4\n","# opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/home/ronen/.conda/envs/nlp/lib/python3.6/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.1.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  UserWarning,\n"],"name":"stderr"},{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'keras_tensor'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-cbcd09a31729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rectified Adam 옵티마이저 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 총 batch size * 4 epoch = 2344 * 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRectifiedAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5.0e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2344\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_proportion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/envs/nlp/lib/python3.6/site-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/envs/nlp/lib/python3.6/site-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/envs/nlp/lib/python3.6/site-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.conda/envs/nlp/lib/python3.6/site-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# TODO: Remove once https://github.com/tensorflow/tensorflow/issues/44613 is resolved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'keras_tensor'"]}]},{"cell_type":"code","metadata":{"id":"05IJJzrs8A6m"},"source":["learning_rate = 0.001\n","batch_size = 100\n","\n","lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate, 2344*4, \n","                                                          0.5, staircase=True)\n","opt = tf.keras.optimizers.Adam(learning_rate=lr_decay)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpLyEPcS8A6n"},"source":["sentiment_drop = tf.keras.layers.Dropout(0.5)(bert_outputs)\n","sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n","sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n","sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymcAQryG8A6n","outputId":"d3219118-14d5-483c-9cb3-6937e6ab6908"},"source":["sentiment_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_word_ids (InputLayer)     [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","input_masks (InputLayer)        [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","input_segment (InputLayer)      [(None, 64)]         0                                            \n","__________________________________________________________________________________________________\n","tf_bert_model (TFBertModel)     ((None, 64, 768), (N 92186880    input_word_ids[0][0]             \n","                                                                 input_masks[0][0]                \n","                                                                 input_segment[0][0]              \n","__________________________________________________________________________________________________\n","dropout_37 (Dropout)            (None, 768)          0           tf_bert_model[0][1]              \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 1)            769         dropout_37[0][0]                 \n","==================================================================================================\n","Total params: 92,187,649\n","Trainable params: 92,187,649\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-KCGNJ908A6n","outputId":"a8b76a88-b3f1-4a50-ee70-4f79ada500fc"},"source":["sentiment_model.fit(train_x, train_y, epochs=15, shuffle=True, batch_size=64, validation_data=(test_x, test_y))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train on 150000 samples, validate on 50000 samples\n","Epoch 1/15\n","150000/150000 [==============================] - 714s 5ms/sample - loss: 0.6935 - accuracy: 0.4997 - val_loss: 0.6933 - val_accuracy: 0.5035\n","Epoch 2/15\n"," 21760/150000 [===>..........................] - ETA: 9:05 - loss: 0.6934 - accuracy: 0.5024"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qKUo5joE8A6n"},"source":[""],"execution_count":null,"outputs":[]}]}