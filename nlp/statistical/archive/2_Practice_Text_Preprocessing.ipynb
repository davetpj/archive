{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['영등포구청역', '에', '있', '는', '맛집', '좀', '알려', '주', '세요', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from pykospacing import Spacing\n",
    "mecab = Mecab()\n",
    "print(mecab.morphs(u'영등포구청역에 있는 맛집 좀 알려주세요.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLGhbEiOoAR7"
   },
   "source": [
    "# 텍스트 전처리 (Text Preprocessing)\n",
    "\n",
    "*   텍스트를 자연어 처리를 위해 용도에 맞도록 사전에 표준화 하는 작업\n",
    "*   텍스트 내 정보를 유지하고, 중복을 제거하여 분석 효율성을 높이기 위해 전처리를 수행\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E585k45HDx5E"
   },
   "source": [
    "### 1) 토큰화 (Tokenizing)\n",
    "* 텍스트를 자연어 처리를 위해 분리 하는 것을\n",
    "* 토큰화는 단어별로 분리하는 \"단어 토큰화(Word Tokenization)\"와 문장별로 분리하는 \"문장 토큰화(Sentence Tokenization)\"로 구분\n",
    "\n",
    "(이후 실습에서는 단어 토큰화를 \"토큰화\"로 통일하여 칭하도록 한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "senwNSwgDzQc"
   },
   "source": [
    "### 2) 품사 부착(PoS Tagging)\n",
    "* 각 토큰에 품사 정보를 추가\n",
    "* 분석시에 불필요한 품사를 제거하거나 (예. 조사, 접속사 등) 필요한 품사를 필터링 하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R15ri5czDyzc"
   },
   "source": [
    "### 3) 개체명 인식 (NER, Named Entity Recognition)\n",
    "* 각 토큰의 개체 구분(기관, 인물, 지역, 날짜 등) 태그를 부착\n",
    "* 텍스트가 무엇과 관련되어있는지 구분하기 위해 사용\n",
    "* 예를 들어, 과일의 apple과 기업의 apple을 구분하는 방법이 개체명 인식임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfq99EkzD1Tk"
   },
   "source": [
    "### 4) 원형 복원 (Stemming & Lemmatization)\n",
    "* 각 토큰의 원형 복원을 함으로써 토큰을 표준화하여 불필요한 데이터 중복을 방지 (=단어의 수를 줄일수 있어 연산을 효율성을 높임)\n",
    "* 어간 추출(Stemming) : 품사를 무시하고 규칙에 기반하여 어간을 추출\n",
    "* 표제어 추출 (Lemmatization) : 품사정보를 유지하여 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5HQOjRvDxmd"
   },
   "source": [
    "### 5) 불용어 처리 (Stopword)\n",
    "* 자연어 처리를 위해 불필요한 요소를 제거하는 작업\n",
    "* 불필요한 품사를 제거하는 작업과 불필요한 단어를 제거하는 작업으로 구성\n",
    "* 불필요한 토큰을 제거함으로써 연산의 효율성을 높임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaIYJczuaS0n"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KysKAL3VlgQN"
   },
   "source": [
    "# 1 영문 전처리 실습\n",
    "\n",
    "\n",
    "NLTK lib (https://www.nltk.org/) 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mND0us3Jppcu"
   },
   "source": [
    "## 1.1 실습용 영문기사 수집\n",
    "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
    "\n",
    "https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/\"\n",
    "res = requests.get(url, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "soup =BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = soup.select('p')\n",
    "text = article[3].get_text()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lB4TzdVQHAsN"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv0ASXb8qa6H"
   },
   "source": [
    "## 1.2 영문 토큰화\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(\"Good muffins cost $3.88\\nin New York\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_tokens = WordPunctTokenizer().tokenize(\"Good muffins cost $3.88\\nin New York\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "word_tokens = TreebankWordTokenizer().tokenize(\"Good muffins cost $3.88\\nin New York\")\n",
    "print(word_tokens)\n",
    "\n",
    "\n",
    "## 또는 spicy 를 사용하여 tokenize 를 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Z-0Nnysqnq"
   },
   "source": [
    "## 1.3 영문 품사 부착 (PoS Tagging)\n",
    "분리한 토큰마다 품사를 부착한다\n",
    "\n",
    "https://www.nltk.org/api/nltk.tag.html\n",
    "\n",
    "태크목록 : https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('muffins', 'NNS'), ('cost', 'VBP'), ('$', '$'), ('3.88', 'CD'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "tagged = pos_tag(word_tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDo-5-khs5Oz"
   },
   "source": [
    "## 1.4 개체명 인식 (NER, Named Entity Recognition)\n",
    "\n",
    "http://www.nltk.org/api/nltk.chunk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"words\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "# 한국어는 선택사항인듯..\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Good/JJ)\n",
      "  muffins/NNS\n",
      "  cost/VBP\n",
      "  $/$\n",
      "  3.88/CD\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "ne_token = ne_chunk(tagged)\n",
    "print(ne_token)\n",
    "# spicy 는 앞의 과정을 한번에 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHjV0h0ZtM-t"
   },
   "source": [
    "## 1.5 원형 복원\n",
    "각 토큰의 원형을 복원하여 표준화 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2eCnbChtXjo"
   },
   "source": [
    "### 1.5.1 어간추출 (Stemming)\n",
    "\n",
    "* 규칙에 기반 하여 토큰을 표준화\n",
    "* ning제거, ful 제거 등\n",
    "\n",
    "https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "규칙상세 : https://tartarus.org/martin/PorterStemmer/def.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"believes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4haNWIcCtZza"
   },
   "source": [
    "### 1.5.2 표제어 추출 (Lemmatization)\n",
    "\n",
    "* 품사정보를 보존하여 토큰을 표준화\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html?highlight=lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#표준화의 한 방법\n",
    "nltk.download(\"wordnet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"believes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmY_SvDMb0fz"
   },
   "source": [
    "## 1.6 불용어 처리 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석을 할 때 제거하는것 \n",
    "# 사전을 활용하는방법\n",
    "stop_pos = ['IN','CC','DT']\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', ','), 3),\n",
       " (('from', 'IN'), 3),\n",
       " (('to', 'TO'), 3),\n",
       " (('and', 'CC'), 3),\n",
       " (('in', 'IN'), 3),\n",
       " (('.', '.'), 3),\n",
       " (('job', 'NN'), 2),\n",
       " (('the', 'DT'), 2),\n",
       " (('of', 'IN'), 2),\n",
       " (('will', 'MD'), 2),\n",
       " (('now', 'RB'), 2),\n",
       " (('all', 'DT'), 2),\n",
       " (('And', 'CC'), 1),\n",
       " (('yes', 'UH'), 1),\n",
       " (('she', 'PRP'), 1),\n",
       " (('does', 'VBZ'), 1),\n",
       " (('mean', 'VB'), 1),\n",
       " (('everybody', 'NN'), 1),\n",
       " ((\"'s\", 'POS'), 1),\n",
       " (('yours', 'NNS'), 1),\n",
       " (('mine', 'VB'), 1),\n",
       " (('onward', 'VB'), 1),\n",
       " (('role', 'NN'), 1),\n",
       " (('grain', 'NN'), 1),\n",
       " (('farmers', 'NNS'), 1),\n",
       " (('Egypt', 'NNP'), 1),\n",
       " (('pastry', 'NN'), 1),\n",
       " (('chefs', 'NNS'), 1),\n",
       " (('Paris', 'NNP'), 1),\n",
       " (('dog', 'NN'), 1),\n",
       " (('walkers', 'NNS'), 1),\n",
       " (('Oregon', 'NNP'), 1),\n",
       " (('i.e', 'NN'), 1),\n",
       " (('every', 'DT'), 1),\n",
       " (('We', 'PRP'), 1),\n",
       " (('be', 'VB'), 1),\n",
       " (('able', 'JJ'), 1),\n",
       " (('help', 'VB'), 1),\n",
       " (('direct', 'VB'), 1),\n",
       " (('workers', 'NNS'), 1),\n",
       " (('’', 'VBP'), 1),\n",
       " (('actions', 'NNS'), 1),\n",
       " (('behavior', 'NN'), 1),\n",
       " (('with', 'IN'), 1),\n",
       " (('a', 'DT'), 1),\n",
       " (('new', 'JJ'), 1),\n",
       " (('degree', 'NN'), 1),\n",
       " (('intelligence', 'NN'), 1),\n",
       " (('that', 'WDT'), 1),\n",
       " (('comes', 'VBZ'), 1),\n",
       " (('predictive', 'JJ'), 1),\n",
       " (('analytics', 'NNS'), 1),\n",
       " (('stemming', 'VBG'), 1),\n",
       " (('AI', 'NNP'), 1),\n",
       " (('engines', 'VBZ'), 1),\n",
       " (('we', 'PRP'), 1),\n",
       " (('increasingly', 'RB'), 1),\n",
       " (('depend', 'VBP'), 1),\n",
       " (('upon', 'NN'), 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = pos_tag(word_tokenize(text))\n",
    "Counter(tagged).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'yours', 'to', 'mine', 'onward', 'to', 'role', 'grain', 'farmers', 'Egypt', ',', 'pastry', 'chefs', 'Paris', 'dog', 'walkers', 'Oregon', 'i.e', '.', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'workers', '’', 'actions', 'behavior', 'new', 'degree', 'intelligence', 'that', 'comes', 'predictive', 'analytics', ',', 'stemming', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for tag in tagged:\n",
    "    if not tag[1] in stop_pos:\n",
    "        words.append(tag[0])\n",
    "        \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QV0orUsOb6wD"
   },
   "source": [
    "## 1.7 영문 텍스트 전처리 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMErzPcbuYEa"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0Dhqm4zkHXl"
   },
   "source": [
    "# 2 한글 전처리 실습\n",
    "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIkGxDnNimek"
   },
   "source": [
    "## 2.1 실습용 한글기사 수집\n",
    "온라인 기사를 바로 수집하여 실습데이터로 사용\n",
    "\n",
    "http://news.chosun.com/site/data/html_dir/2018/07/10/2018071004121.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=110&oid=023&aid=0003386456\"\n",
    "headers = {'user-agent':'Mozilla/5.0'}\n",
    "res = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = soup.select(\"#articleBodyContents\")[0].text\n",
    "\n",
    "type(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w09FHRgIphw5"
   },
   "source": [
    "## 2.2 한글 토큰화 및 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IZWN4xX4HXW"
   },
   "source": [
    "한글 자연어처리기 비교\n",
    "\n",
    "https://konlpy.org/ko/latest/morph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나나', '나', '나', '나나', '나나', '나']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"나나나 나 나나나나나 나나나 나 나나나나나 나나나 나 나나나나나\"\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "tagged = komoran.morphs(text)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7nyptjunTG"
   },
   "source": [
    "## 2.3 한글 품사 부착 (PoS Tagging)\n",
    "\n",
    "PoS Tag 목록\n",
    "\n",
    "https://docs.google.com/spreadsheets/u/1/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = mecab.pos(article)\n",
    "tagged=pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZY4s8tbuuXP"
   },
   "source": [
    "## 2.4 불용어(Stopword) 처리\n",
    "분석에 불필요한 품사를 제거하고, 불필요한 단어(불용어)를 제거한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    for line in f:\n",
    "        stopwords.append(line.strip())\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_pos = ['EC','EP','SC','JK','JKO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "print(len(tagged))\n",
    "words=[]\n",
    "for tag in tagged:\n",
    "    if tag[0] in stopwords or tag[1] in stop_pos:\n",
    "        continue\n",
    "    words.append(tag[0])\n",
    "    \n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1BDV2EAzug6"
   },
   "source": [
    "# 2 N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05 KS Practice. Text Preprocessing",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
