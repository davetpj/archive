{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1Q9mWDrgMUn"
   },
   "source": [
    "# Text Summarization :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSZIeetL8Yym"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:57.110994Z",
     "start_time": "2021-07-10T15:52:56.568733Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "_OHg2GEi7ZKI",
    "outputId": "0a1378b7-66be-4efb-de97-1fb8148436af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 국가 차원의 빅데이터 활용 시대가 열린다. 새로운 산업 창출과 기존 산업의 변화에 이르는\n",
      " 혁신하고 기업의 경쟁력을 한 단계 제고할 수 있도록 정책적 역량을 집중하겠다”고 밝혔다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def cleansing(soup):\n",
    "    body_content = soup.select_one(\"#articleBodyContents\")\n",
    "    content = body_content.get_text()\n",
    "    for strong in body_content.select(\"strong\"):\n",
    "        content = content.replace(strong.get_text(), \"\")\n",
    "\n",
    "    for td in body_content.select(\"td\"):\n",
    "        content = content.replace(td.get_text(), \"\")\n",
    "\n",
    "    content = re.sub(r\"\\[[가-힣 ]+\\]\", \"\", content)\n",
    "    start_pos = re.search(\n",
    "        r\"[가-힣]{2,4}\\s\\(?\\w+@\\w+\\.\\w+(.\\w+)?\\)\", content).start()\n",
    "    content = content[:start_pos]\n",
    "    content = content.replace(\"\\n\", \"\").replace(\n",
    "        \"// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\", \"\")\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_news_by_url(url):\n",
    "    headers = {\"user-agent\": \"Mozilla/5.0\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    content = cleansing(soup)\n",
    "    return content\n",
    "\n",
    "\n",
    "doc = get_news_by_url(\n",
    "    'https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108')\n",
    "print(doc[:50])\n",
    "print(doc[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpTxmnbx8v6m"
   },
   "source": [
    "## install mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:57.116009Z",
     "start_time": "2021-07-10T15:52:57.114028Z"
    },
    "id": "qsWnZkd78qCA"
   },
   "outputs": [],
   "source": [
    "# !sudo apt-get install g++ openjdk-7-jdk # Install Java 1.7+\n",
    "# !sudo apt-get install python-dev; pip install konlpy     # Python 2.x\n",
    "# !sudo apt-get install python3-dev; pip3 install konlpy   # Python 3.x\n",
    "# !sudo apt-get install curl\n",
    "# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piKvCjRvgHFE"
   },
   "source": [
    "# Luhn Summarizer\n",
    "\n",
    "http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFwBVB3ogZeF"
   },
   "source": [
    "Hans Peter Luhn\n",
    "\n",
    "https://en.wikipedia.org/wiki/Hans_Peter_Luhn\n",
    "\n",
    " Hans Peter Luhn은 공학과 정보과학에서의 개척 작업으로 \"정보 검색의 아버지\"로 알려져 있다. 그는 표제어가 문맥에 포함된 채 배열된 색인(KWIC : keyword-in-context) 개발, 정보 선택 제공(SDI), 완전 텍스트 프로세싱, 자동 발췌(요약), 단어 시소로스의 최초 현대식 사용으로 신뢰를 얻었다. 오늘날 파생된 지식 대부분에는 KWIC 색인이 있으며 과학의 모든 분야에 SDI시스템이 있다. \n",
    "\n",
    "\n",
    " Luhn은 1896년 7월 1일 독일 바르멘에서 태어났다. 아버지가 그 당시 유명한 인쇄업자였으므로, 스위스에서 인쇄업을 배웠다. 어렸을 때무터 그는 창조성이 뛰어난 재능을 보였으며, 기술적 문제, 물리학, 통계학에 관심을 보였다. 그러나 1차 세계대전으로 독일군 통신장교로 복역(1915년1917년)하면서 프랑스 터키 루마니아 불가리아 등지로 옮겨 다녀야만 했다. 1차 대전 이후 그는 스위스의 Gallen 교회 Schweizwrische handels Hochschule로 돌아와 기술, 물리학, 회계분야의 수업을 들었다. 그 후, Luhn은 그리스에서 못다한 공부에 전념했으며, 더블부기기계(Duble-Entry Bookeeping Machine : 카드 대장에 대변기입과 차변 기입을 기록할 수 있는 것)를 발명하였다. 그는 또 Hollerith tabulating/recording machine에 정통했고, 천공카드에서 문자 숫자를 나타내는 장치의 사용을 증가시키게 했다. 1920년부터 1930년까지는 10개의 특허권을 획득하여 그의 탁월한 능력을 보여주었다. 그것들중에 루노메터(Lunometer : 직물의 실길이를 계산하는데 쓰이는 장치)는 지금도 사용되고 있다.\n",
    "\n",
    "\n",
    " 1920년까지 그는 직물 공장에서 일하기 시작했다. 그는 직물 공장의 사업확장을 위해 뉴잉글랜드에서 1924년 미국으로 가게 되었다. 그러나 회사가 곧 파산하였고 Luhn은 직장없이 뉴욕에 남게 되었다. 그는 은행에서 일을 하였고 곧 뉴욕 월스트리트에 소재한 국제어음은행(International Acceptance Bank)에서 재정담당관으로 승진하였다. \n",
    "\n",
    "\n",
    " 1933년 Luhn은 자사인 공학회사 H.P. Luhn & Association을 설립하였고 8년간 자문기술자로 일했다. 1941년 Luhn은 IBM에서 수석 연구기술자로 참여하였고 이후에 정보검색연구 관리자로 일했다. Luhn이 IBM에서 새로운 아이디어를 지속적으로 내놓고 문제를 다르게 접근하여 주목을 받는 동안, 다른 기술자들에게 고차원적인 창조를 하도록 자극하면서 그들의 촉매제 역할을 하여 신뢰를 얻었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:57.120854Z",
     "start_time": "2021-07-10T15:52:57.118975Z"
    },
    "id": "GbuQON2YmWpV"
   },
   "outputs": [],
   "source": [
    "#doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.173877Z",
     "start_time": "2021-07-10T15:52:57.123130Z"
    },
    "id": "7IIA2vf5kbq9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPM0f2uh5eq0"
   },
   "source": [
    "### 1) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.178627Z",
     "start_time": "2021-07-10T15:52:58.176425Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.300351Z",
     "start_time": "2021-07-10T15:52:58.180608Z"
    },
    "id": "xRDgb1Ksd3VH"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from kss import split_sentences\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "# 문장 분리\n",
    "def get_sentences(txt):\n",
    "    return split_sentences(txt)\n",
    "\n",
    "# 토큰화\n",
    "def get_words(txt):\n",
    "    return [token[0] for token in mecab.pos(txt) if token[1][0] == 'N' and len(token[0]) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsVUL-Bg5sIw"
   },
   "source": [
    "### 2) 중요 단어 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.313086Z",
     "start_time": "2021-07-10T15:52:58.305678Z"
    },
    "id": "w6X3Kwj_oIRh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'사과', '포도'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 단어(토큰)의 가중치 계산 및 범위에 포함되는 토큰 식별\n",
    "# def get_keywords(word_list , min_ratio=0.001, max_ratio=0.5) :\n",
    "\n",
    "#     #조건에 맞지 않으면 에러를 낸다.\n",
    "#     assert (min_ratio < 1 and max_ratio < 1)\n",
    "#     pass\n",
    "#     # 토큰별로 빈도수 카운팅\n",
    "#     count_dict = {}\n",
    "#     for word in word_list:\n",
    "#         # 방법 1\n",
    "#         if word in count_dict.keys():\n",
    "#             count_dict[word] += 1\n",
    "#         else:\n",
    "#             count_dict[word] = 1\n",
    "\n",
    "#         # 방법 2\n",
    "# #         count_dict.setdefault(word, 0)\n",
    "# #         count_dict[word] +=1\n",
    "\n",
    "\n",
    "#     # 분석 문서의 총 토큰수 대비 해당 토큰의 빈도 비율\n",
    "#     keywords = set()\n",
    "#     for word, cnt in count_dict.items():\n",
    "#         word_percentage=cnt/len(word_list)\n",
    "\n",
    "#         # 사전 정의한 비율내에 포함 된 경우 키워드에 추가\n",
    "#         if word_percentage >= min_ratio and word_percentage <= max_ratio:\n",
    "#             #중복을 없애기 위해 set 을 사용 set 에서의 append 는 add\n",
    "#             keywords.add(word)\n",
    "#     return keywords\n",
    "\n",
    "# get_keywords(['바나나','사과','바나나','바나나','포도'])\n",
    "\n",
    "\n",
    "# 단어(토큰)의 가중치 계산 및 범위에 포함되는 토큰 식별\n",
    "def get_keywords(word_list, min_ratio=0.001, max_ratio=0.5):\n",
    "    assert (min_ratio < 1 and max_ratio < 1)\n",
    "\n",
    "    # 토큰별로 빈도수 카운팅\n",
    "    count_dict = {}\n",
    "    for word in word_list:\n",
    "        # if word in count_dict.keys():\n",
    "        #     count_dict[word] += 1\n",
    "        # else:\n",
    "        #     count_dict[word] = 1\n",
    "        count_dict.setdefault(word, 0)\n",
    "        count_dict[word] += 1\n",
    "\n",
    "    # 분석 문서의 총 토큰수 대비 해당 토큰의 빈도 비율\n",
    "    keywords = set()\n",
    "    for word, cnt in count_dict.items():\n",
    "        word_percentage = cnt/len(word_list)\n",
    "\n",
    "        # 사전 정의한 비율내에 포함 된 경우 키워드에 추가\n",
    "        if word_percentage >= min_ratio and word_percentage <= max_ratio:\n",
    "            keywords.add(word)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "\n",
    "get_keywords(['바나나', '사과', '바나나', '바나나', '포도'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmEk0zbd5vcx"
   },
   "source": [
    "### 3) 문장 중요도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.321494Z",
     "start_time": "2021-07-10T15:52:58.316308Z"
    },
    "id": "K-mVXXWYoK5C"
   },
   "outputs": [],
   "source": [
    "# 문장의 가중치 계산\n",
    "def get_sentence_weight(sentence, keywords):\n",
    "    tokens = sentence.split(\" \")\n",
    "    window_start, window_end = 0, -1\n",
    "\n",
    "    # 문장내에서 윈도 시작 위치 탐색\n",
    "    # 범위내 속한 키워드가 등장하는 첫번째 위치 계산\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in keywords:\n",
    "            window_start = i\n",
    "            break\n",
    "\n",
    "    # 문장내에서 윈도 종료 위치 탐색\n",
    "    # 범위내 속한 키워드가 등장하는 마지막 위치 계산\n",
    "    for i in range(len(tokens) - 1, 0, -1):\n",
    "        if tokens[i] in keywords:\n",
    "            window_end = i\n",
    "            break\n",
    "\n",
    "    # 윈도의 시작위치가 종료위치보다 큰경우 => 분석할 단어(토큰)가 없는 경우 종료\n",
    "    if window_start > window_end:\n",
    "        return 0\n",
    "\n",
    "    # 윈도 크기 계산\n",
    "    window_size = window_end - window_start + 1\n",
    "\n",
    "    # 분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수 카운팅\n",
    "    keyword_cnt = 0\n",
    "    for w in tokens[window_start: window_start + window_size]:\n",
    "        if w in keywords:\n",
    "            keyword_cnt += 1\n",
    "\n",
    "    # (분석 대상 문장 중 범위(0.001 ~ 0.5)에 포함된 토큰 개수) / 윈도사이즈\n",
    "    return keyword_cnt * keyword_cnt * 1.0 / (window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rufxIxIb5zq9"
   },
   "source": [
    "### 4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.328042Z",
     "start_time": "2021-07-10T15:52:58.323862Z"
    },
    "id": "ZntB_Sx0d832"
   },
   "outputs": [],
   "source": [
    "# 문서 요약\n",
    "def summarize(content ,max_no_of_sentences = 10):\n",
    "    \n",
    "    \n",
    "    # 단어(토큰) 분리\n",
    "    word_list = get_words(content)\n",
    "    \n",
    "    # 단어(토큰) 가중치 계산 및 범위 내 포함 단어(토큰) 추출\n",
    "    keywords = get_keywords(word_list)\n",
    "\n",
    "    # 문장별 가중치 계산\n",
    "    sentence_list = get_sentences(content)\n",
    "    sentence_weight = []\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "        sentence_weight.append((get_sentence_weight(sentence, keywords), sentence))\n",
    "           \n",
    "    # 문장별 가중치 역순 계산\n",
    "    sentence_weight.sort(reverse=True)\n",
    "    \n",
    "    return [ weight[1] for weight in sentence_weight[:max_no_of_sentences]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.399386Z",
     "start_time": "2021-07-10T15:52:58.329750Z"
    },
    "id": "mmZxMoce6S3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 ‘빅데이터 플랫폼 및 센터’ 출범식 행사를 개최했다.\n",
      "\n",
      "\n",
      "특히 공공과 민간 사이 데이터 파일형식 등이 달라 호환이 제대로 이뤄지지 못한 문제를 해소하기 위해 개방형 표준을 적용하고, 품질관리기준도 마련해 운영한다.\n",
      "\n",
      "\n",
      "금융 플랫폼의 경우 소상공인 신용평가 고도화 등을 통해 금융 취약 계층 대상 중금리 대출이자를 2%p 절감해 연간 1조원의 신규대출을 창출할 전망이다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "li = summarize (doc ,  3)\n",
    "for s in li :\n",
    "    print(s)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctQ8vU85d2Uy"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUOOmfhngBnb"
   },
   "source": [
    "# Textrank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2cG-zDxW0aW"
   },
   "source": [
    "![대체 텍스트](https://www.researchgate.net/profile/Khushboo_Thakkar3/publication/232645575/figure/fig1/AS:575720050573312@1514273764062/Sample-graph-build-for-sentence-extraction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-wrC-1D_a0V"
   },
   "source": [
    "## TextRank 직접 구현하기 (Matrix 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LJjhauc6Jlz"
   },
   "source": [
    "### 1) 자카드 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.404978Z",
     "start_time": "2021-07-10T15:52:58.401661Z"
    },
    "id": "yu3gn_zOE-fv"
   },
   "outputs": [],
   "source": [
    "Text = \"딸기 바나나 사과 파인애플. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.414530Z",
     "start_time": "2021-07-10T15:52:58.407284Z"
    },
    "id": "ev4s4s0YBxa3"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "# 문장간 유사도 측정 (자카드 유사도 사용)\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1 = [token[0] for token in mecab.pos(sentence1) if token[1][0] in ['N','V']]\n",
    "    sentence2 = [token[0] for token in mecab.pos(sentence2) if token[1][0] in ['N','V']]\n",
    "\n",
    "    union = set(sentence1).union(set(sentence2))\n",
    "    intersection = set(sentence1).intersection(set(sentence2))\n",
    "\n",
    "    \n",
    "\n",
    "sentence_similarity('나는 치킨을 좋아해','나는 치킨을 싫어해')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ma25LRfEx67"
   },
   "source": [
    "### 2) 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.421866Z",
     "start_time": "2021-07-10T15:52:58.416915Z"
    },
    "id": "38ThXVuHB2bA"
   },
   "outputs": [],
   "source": [
    "def buildMatrix(sentences):\n",
    "    score = np.ones(len(sentences), dtype=np.float32)\n",
    "    weighted_edge = np.zeros((len(sentences), len(sentences)), dtype=np.float32)\n",
    "\n",
    "       # 문장별로 그래프 edge를 Matrix 형태로 생성\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "                weighted_edge[i][j] = sentence_similarity(\n",
    "                    sentences[i], sentences[j])\n",
    "\n",
    "        # normalize\n",
    "    for i in range(len(weighted_edge)):\n",
    "        score[i] = weighted_edge[i].sum()\n",
    "        weighted_edge[i] /= score[i]\n",
    "\n",
    "    return weighted_edge\n",
    "\n",
    "\n",
    "Text = \"딸기 바나나 사과 파인애플 수박. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\"\n",
    "#buildMatrix(sent_tokenize(Text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfYbGdLp6wrE"
   },
   "source": [
    "### 3) 문장 중요도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.427747Z",
     "start_time": "2021-07-10T15:52:58.423982Z"
    },
    "id": "FlAJ0nVaB36Z"
   },
   "outputs": [],
   "source": [
    "def scoring(weight_edge, score, threshold=0.0001, d=0.85, max_iter = 50):\n",
    "    for iter in range(max_iter):\n",
    "        new_score = (1 - d) + d * weight_edge.T.dot(score)\n",
    "        if abs(new_score - score).sum() <= threshold:\n",
    "            break\n",
    "\n",
    "        score = new_score\n",
    "\n",
    "    return new_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJBLc2jG60OR"
   },
   "source": [
    "### 4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.434443Z",
     "start_time": "2021-07-10T15:52:58.429471Z"
    },
    "id": "SkIdmjkNBZfz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.441240Z",
     "start_time": "2021-07-10T15:52:58.436075Z"
    },
    "id": "1DVvQdjSB5PE"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def summarize(text, n=10):\n",
    "    #   sentences = sent_tokenize(text)\n",
    "    sentences = get_sentences(text)\n",
    "\n",
    "    weighted_edge = buildMatrix(sentences)\n",
    "    init_score = np.ones(len(sentences), dtype=np.float32)\n",
    "    score = scoring(weighted_edge, init_score)\n",
    "\n",
    "    sorted_score = sorted(\n",
    "        enumerate(score), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [sentences[sent[0]] for sent in sorted_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.454067Z",
     "start_time": "2021-07-10T15:52:58.444302Z"
    },
    "id": "EqjLqMOyR18k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딸기 바나나 사과 파인애플 수박. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-5f1bfb1f64f7>:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  weighted_edge[i] /= score[i]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(Text, 3)\n",
    "\n",
    "for sent in summary:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.513654Z",
     "start_time": "2021-07-10T15:52:58.456286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국가 차원의 빅데이터 활용 시대가 열린다.\n",
      "새로운 산업 창출과 기존 산업의 변화에 이르는 ‘혁신성장’을 위한 센터가 문을 연다.\n",
      "10개 분야에 걸쳐 ‘데이터 경제’의 발전을 위한 정부의 청사진을 현실로 구현하는데 앞장선다는 계획이다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-5f1bfb1f64f7>:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  weighted_edge[i] /= score[i]\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(doc, 3)\n",
    "\n",
    "for sent in summary:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J16o9Y89WgNT"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8Yj0ioJimhl"
   },
   "source": [
    "## TextRank 직접 구현하기 (Graph 활용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.518697Z",
     "start_time": "2021-07-10T15:52:58.515752Z"
    },
    "id": "I9kctoHX8RJm"
   },
   "outputs": [],
   "source": [
    "Text = \"딸기 바나나 사과 딸기 파인애플. 바나나 사과 딸기. 복숭아 파인애플. 파인애플 사과 딸기 바나나.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-1Rfaqx7DFN"
   },
   "source": [
    "### 2) 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.527671Z",
     "start_time": "2021-07-10T15:52:58.522497Z"
    },
    "id": "yWefbGlx8W3-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/dhkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:52:58.547344Z",
     "start_time": "2021-07-10T15:52:58.530750Z"
    },
    "id": "wUAak7ej8IVp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['딸기 바나나 사과 딸기 파인애플.', '바나나 사과 딸기.', '복숭아 파인애플.', '파인애플 사과 딸기 바나나.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# nltk 는 . 기준으로 토큰화 함을 알 수 있다.\n",
    "sentences(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtB08RJ8Ixa"
   },
   "source": [
    "### 3) 자카드 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:54:10.260682Z",
     "start_time": "2021-07-10T15:54:10.251221Z"
    },
    "id": "vU3qw-rBpiOc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "# 문장간 유사도 측정 (자카드 유사도 사용)\n",
    "# ( 문장 1 - 문장2 유사도) / (문장1 전체 유사도 합(0.5 / (0.5 + 0.8 + 0.167) -> 이거 자체가 score!\n",
    "# 유사도를 기반으로, 그냥 초기 스코어를 edge 유사도의 합으로 구해(들어온거)\n",
    "# 그 다음 egge 가중치\n",
    "\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    sentence1 = mecab.morphs(sentence1)\n",
    "    sentence2 = mecab.morphs(sentence2)\n",
    "\n",
    "    union = set(sentence1).union(set(sentence2))\n",
    "    intersection = set(sentence1).intersection(set(sentence2))\n",
    "\n",
    "    #print(union)\n",
    "    #print(intersection)\n",
    "    # 교집합의 길이 / 합집합의 길이\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "\n",
    "sentence_similarity('나는 치킨을 좋아해', '나는 치킨을 싫어해')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:54:10.815606Z",
     "start_time": "2021-07-10T15:54:10.812292Z"
    },
    "id": "zuOJL2EW-uII"
   },
   "outputs": [],
   "source": [
    "def connect(nodes):\n",
    "    return [(start, end, sentence_similarity(start,end)) for start in nodes for end in nodes if start is not end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbxnM_Cd7Fri"
   },
   "source": [
    "### 3) 그래프 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:54:11.549887Z",
     "start_time": "2021-07-10T15:54:11.545527Z"
    },
    "id": "rKEErtAU-tmj"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def rank(nodes,edges):\n",
    "    graph=nx.diamond_graph()\n",
    "    graph.clear() \n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_weighted_edges_from(edges)\n",
    "\n",
    "    return nx.pagerank(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysIrWFvG7LX4"
   },
   "source": [
    "### 4) 문서 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:54:12.319281Z",
     "start_time": "2021-07-10T15:54:12.314434Z"
    },
    "id": "6XQkxYkv-zxI"
   },
   "outputs": [],
   "source": [
    "def summarize(text,num_summaries=3):\n",
    "    \n",
    "    # nltk <-- . 기준 토큰화\n",
    "    nodes = sentences(text)\n",
    "    #print(nodes) \n",
    "\n",
    "    edges = connect(nodes)\n",
    "    scores = rank(nodes, edges)\n",
    "    #print(scores) # 문장에 대한 score\n",
    "     \n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:num_summaries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-10T15:54:12.861506Z",
     "start_time": "2021-07-10T15:54:12.856525Z"
    },
    "id": "UwlnlxyP8VM7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('딸기 바나나 사과 딸기 파인애플.', 0.3034803909925603)\n",
      "('파인애플 사과 딸기 바나나.', 0.3034803909925603)\n",
      "('바나나 사과 딸기.', 0.254517723082264)\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(Text, 3)\n",
    "\n",
    "for sent in summary:\n",
    "    print(sent)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "09 Prac. Document Summarization",
   "provenance": []
  },
  "interpreter": {
   "hash": "7fc8111f0d3ffb4406a7bec14f9173c23bfb85d13f5260fce74791c0ea7fd588"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
